{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ca4c32-646c-4fdd-9e5e-260cb0ad4df3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database_name: abtest_workshop_sixuan_he\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, round\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "import random\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5c5fbf-edef-495a-8a19-c1ab8d9bf387",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------+-----------+---------------+\n|listing_id|  region|review_score|create_date|number_of_rooms|\n+----------+--------+------------+-----------+---------------+\n|         1|Region_0|       86.41|  2023-01-1|              1|\n|         2|Region_1|       75.91|  2023-01-2|              2|\n|         3|Region_2|       89.08|  2023-01-3|              3|\n|         4|Region_3|       76.62|  2023-01-4|              4|\n|         5|Region_4|       76.85|  2023-01-5|              5|\n|         6|Region_5|       79.71|  2023-01-6|              1|\n|         7|Region_6|       86.15|  2023-01-7|              2|\n|         8|Region_7|       87.88|  2023-01-8|              3|\n|         9|Region_8|       76.85|  2023-01-9|              4|\n|        10|Region_9|       79.66| 2023-01-10|              5|\n|        11|Region_0|       82.55| 2023-01-11|              1|\n|        12|Region_1|       83.24| 2023-01-12|              2|\n|        13|Region_2|       76.97| 2023-01-13|              3|\n|        14|Region_3|       84.51| 2023-01-14|              4|\n|        15|Region_4|        83.9| 2023-01-15|              5|\n|        16|Region_5|       81.54| 2023-01-16|              1|\n|        17|Region_6|       76.47| 2023-01-17|              2|\n|        18|Region_7|       83.36| 2023-01-18|              3|\n|        19|Region_8|       72.11| 2023-01-19|              4|\n|        20|Region_9|       77.07| 2023-01-20|              5|\n+----------+--------+------------+-----------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder.appName(\"AirbnbSimulator\").getOrCreate()\n",
    "\n",
    "# Define the number of rows for the simulated dataset\n",
    "num_rows = 1000\n",
    "\n",
    "# Generate the simulated data\n",
    "data = []\n",
    "for i in range(num_rows):\n",
    "    listing_id = i + 1\n",
    "    region = f\"Region_{i % 10}\"\n",
    "    review_score = round(random.gauss(80, 5), 2)  # Generating review scores from 10 to 100\n",
    "    create_date = f\"2023-01-{(i % 30) + 1}\"  # Creating dates within January 2023\n",
    "    number_of_rooms = (i % 5) + 1\n",
    "    \n",
    "    data.append((listing_id, region, review_score, create_date, number_of_rooms))\n",
    "\n",
    "# Create a DataFrame from the simulated data\n",
    "listing_df = spark.createDataFrame(data, [\"listing_id\", \"region\", \"review_score\", \"create_date\", \"number_of_rooms\"])\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "listing_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d8d12c-4afc-41e4-b1db-19731fd6c44a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- listing_id: long (nullable = true)\n |-- region: string (nullable = true)\n |-- review_score: double (nullable = true)\n |-- create_date: string (nullable = true)\n |-- number_of_rooms: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "listing_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e08c6c-b155-4d63-a272-c93a18ef8404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|listing_id|\n+----------+\n|       814|\n|       404|\n|        74|\n|       224|\n|        64|\n|       294|\n|       394|\n|       924|\n|       364|\n|       344|\n|       914|\n|       554|\n|       164|\n|       884|\n|       984|\n|       674|\n|       624|\n|       704|\n|       634|\n|       734|\n+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Define the function to rank listings by region\n",
    "def rank_listings_by_region(area,spark):\n",
    "    # Load the listings DataFrame from the specified table\n",
    "    spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW listings_df AS SELECT * FROM listing_df\")\n",
    "    # Apply filters and sorting using SQL query\n",
    "    ranked_listings = spark.sql(f\"\"\"\n",
    "        SELECT listing_id\n",
    "        FROM listings_df\n",
    "        WHERE region = '{area}'\n",
    "        ORDER BY review_score DESC\n",
    "    \"\"\")\n",
    "    return ranked_listings\n",
    "\n",
    "\n",
    "# Create a temp table from DataFrame generated for example\n",
    "listing_df.createOrReplaceTempView(\"listing_df\")\n",
    "# Call the function to rank listings by region\n",
    "ranked_listings = rank_listings_by_region(\"Region_3\", spark)\n",
    "# Show the ranked listings\n",
    "ranked_listings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d16718c-e3fc-4143-b47d-1ec5298f6d06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    listing_id\n0          814\n1          404\n2           74\n3          224\n4           64\n..         ...\n95         804\n96         484\n97         384\n98         794\n99         934\n\n[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define your SimplifiedRandomModel class that inherits from mlflow.pyfunc.PythonModel\n",
    "class RankingModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input):\n",
    "        # Initialize Spark session\n",
    "        spark = SparkSession.builder.appName('RankingModel').getOrCreate()\n",
    "        # Extract region from the model input dictionary (not used in this case)\n",
    "        region = model_input[\"region\"][0]\n",
    "        # Call the rank_listings_by_region function\n",
    "        return rank_listings_by_region(region, spark).toPandas()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the RankingModel class\n",
    "    model = RankingModel()\n",
    "\n",
    "    # Define model_input as a dictionary (region is not used in this case)\n",
    "    model_input = {\"region\": [\"Region_3\"], 'user_id': [100]}\n",
    "\n",
    "    # Call the predict method of the model\n",
    "    result = model.predict(None, model_input)\n",
    "\n",
    "    # Print the resulting ranked listings\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56b47e8-b6ac-486f-9ad8-2f876542b5dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a sample test DataFrame\n",
    "import pandas as pd\n",
    "test_data = pd.DataFrame({\n",
    "    'region': ['Region_2'],\n",
    "    'user_id': [100]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647f6f61-ec06-4f77-bf91-3ea46b76b8c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    listing_id\n",
       "0          383\n",
       "1          463\n",
       "2          473\n",
       "3            3\n",
       "4           73\n",
       "..         ...\n",
       "95         943\n",
       "96         643\n",
       "97         893\n",
       "98         933\n",
       "99         263\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Initialize MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Create an instance of the SimplifiedRandomModel\n",
    "model = RankingModel()\n",
    "model.predict(None, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f56e65-e1f0-42ce-b7aa-a87229a90416",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/mlflow/models/signature.py:152: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  inputs = _infer_schema(model_input)\n/databricks/python/lib/python3.10/site-packages/mlflow/models/signature.py:153: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  outputs = _infer_schema(model_output) if model_output is not None else None\n2023/09/06 02:16:50 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpsze9_8ca/model, flavor: python_function), fall back to return ['cloudpickle==2.0.0']. Set logging level to DEBUG to see the full traceback.\nRegistered model 'sql_model' already exists. Creating a new version of this model...\n2023/09/06 02:16:51 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: sql_model, version 2\nCreated version '2' of model 'sql_model'.\n"
     ]
    }
   ],
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "    # Call the `predict()` method on the instantiated model with the required arguments\n",
    "    prediction = model.predict(context=None, model_input=test_data)\n",
    "\n",
    "    # Infer the signature of the predict function\n",
    "    signature = infer_signature(test_data, prediction)\n",
    "\n",
    "    # Log the model artifact to MLflow\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"sql_model\",\n",
    "        python_model=model,\n",
    "        input_example=test_data,\n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "    # Register the model to the model registry\n",
    "    mv = mlflow.register_model(f'runs:/{run.info.run_id}/sql_model', \"sql_model\")\n",
    "    client.transition_model_version_stage(f'sql_model', mv.version, \"Production\", archive_existing_versions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e267717-c96f-4c22-811d-57d7dc96de97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    listing_id\n0          882\n1          822\n2          352\n3          872\n4          292\n..         ...\n95         842\n96         832\n97         622\n98          52\n99         662\n\n[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the registered model\n",
    "model_uri = \"models:/sql_model/2\"  # Update with the correct URI for your model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Create a new test DataFrame\n",
    "new_test_data = pd.DataFrame({\n",
    "    'region': ['Region_1'],\n",
    "    'user_id': [300]\n",
    "})\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(new_test_data)\n",
    "\n",
    "# Display the predictions\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2488707797425907,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "SQL-based MLflow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
